---
date: 2024-05-18
type: ðŸ§ 
tags:
  - MAC/6/PE
aliases:
  - "policy"
---

**Topics:** [[Markov Decision Process]]

---

_**(definition)**_

In the context of a [[Markov Decision Process|Markov decision process]] (MDP), a **policy** is a [[Function|mapping]] of every one of the MDP's [[State Set|states]] to a a specific decision. 

In other words, a **policy** tells us which decisions are taken on which state. 

Policies are normally denoted by $r$, while the set of possible policies in a given MDP is denoted by $R$. 

We can denote the decision taken in a state $i$ according to the policy $r$ with $d_{i}\{r\}$:

$$
r = \left(d_{0}\{r\}, d_{1}\{r\}, \dots \right)
$$
